# 一、概述

机器学习的三个主要元素

- 数据
- 模型
- 模型评估（损失函数）
  - Hingle Loss
  - Logstic Loss
  - Softmax Loss

数据特征

- 特征数量大
- 稀疏（大多特征值为0）

# 二、线性回归

损失函数公式：
$$
L_D(w)=\sum_{i=1}^n l(y_i'\, ,\, y_i)
$$

$$
l(y_i' \, , \, \, y_i)=\frac 1 2\,(y_i'-y_i)^2
$$

模型函数：
$$
\begin{align}
f(x;w,b)&=w_1x_1+\cdots+w_mx_m+b\\
&=\sum_{i=1}^mw_ix_i+b\\
&=w^Tx+b
   \end{align}
$$
m是特征数量
$$
\begin{align}
L_D(w)&=\frac 1 2 \|	Y-Xw\|_2^2\\
      &=\frac 1 2 (y-Xw)^T\,(y-Xw)\\
      \end{align}
$$
X指特征x的矩阵形式

对上述损失函数求导微分： 令y-Xw=a
$$
\begin {align}
\frac {\partial L_D(w)}{\partial w}&=\frac {\partial a}{\partial w} \frac{\partial \frac 1 2 a^Ta}{\partial a}\\
&=\frac  1 2\frac {\partial a}{\partial w} (2a)\\
&=\frac {\partial (y-Xw)}{\partial w}(y-Xw)\\
&=-X^T(y-Xw)
\end {align}
$$
令上述式子为0：
$$
w^*=(X^TX)^{-1}X^Ty
$$

缺点：不可逆

##### 梯度下降

梯度：
$$
d=-\frac{\partial L_D(w)}{\partial w}
$$

$$
w'=w+\eta d
$$
控制好学习率：

- 过大 摇摆甚至偏离下降
- 过小 过程太慢甚至不能聚合

$$
开始使用大学习率 逐步下降\\
\eta_{k+1}=\frac{\eta_k}{k+1}
$$

# 三、支持向量机SVM

![image-20211205172651603](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211205172651603.png)
$$
f(x)=
\begin{cases}
\le0 ,&\,y_i=+1\\
\ge0 ,&\,y_i=-1
\end {cases}
$$
若给定数据集线性可分 必存在一个平面
$$
w^Tx+b=0
$$
将两类样本分开 则距离为：
$$
d=\frac{w^Tx+b}{\|w\|}\\
其中\|w\|=\sqrt{w_1^2+\cdots+w_n^2}
$$
推导：

![image-20211205173712967](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211205173712967.png)

所以问题即：
$$
max_{w,b}\frac 2 {\|w\|} \ \ s.t.w^Tx+b=
\begin{cases}
\ge1 \ \ \ y_i=+1\\
\le1 \ \ \ y_i=-1\\
\end{cases}
$$
等价于
$$
min_{w,b}\frac{\|w\|^2}{2}\\
s.t.\ \ \ \ \ y_i(w^Tx_i+b)\ge1\ \ \ \ \ \ \ \ \ \ i=1,2,3,\cdots,n
$$

##### 软间隔分类

引入松弛变量$\xi_i$ 代表有许多样本分在错误一类

- $\xi_i$ =0代表分类非常完美
- $0<\xi_i<1$ 代表分类正确但是比最完美的d小
- $\xi_i\ge1$ 代表分类错误

$$
原始变为min_{w,b,\xi_i}\frac{\|w\|^2}{2}+\frac{C}{n}\sum_{i=1}^n\xi_i\\
s.t.\,\,\ \ \ \ \ \ y_i(w^Tx_i+b)\ge1-\xi_i,\ \ \ \forall\xi_i\ge0, \ \ i=1,2,3\cdots.n
$$

where C is a hyperparameter:(超变量) 

-small C makes constraints easy to be ignored 

-large C makes constraints hard to be ignored



（前提：$\xi_i\ge0$ )

[^]: 

Hingle Loss(max部分）:

整体是问题的目标
$$
\min_{w,b}\frac{\|w\|^2}{2}+\frac{C}{n}\sum_{i=1}^{n}\max(0,1-y_i(w^Tx_i+b))
$$

#### 梯度下降

对$\xi_i$ 求w微分
$$
\begin{align}
\xi_i&\ge0:\\
g_w(x_i)&=\frac{\partial(-y_i(w^Tx_i+b))}{\partial w}\\
&=-\frac{y_iw^Tx_i}{\partial w}\\
&=-y_ix_i\\


\xi_i&<0:\\
g_w(x_i)&=0
\end{align}
$$
求b微分同理：

则有：

<img src="C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211206141007421.png" alt="image-20211206141007421" style="zoom:80%;" />

![image-20211206141251014](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211206141251014.png)

![image-20211206141326276](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211206141326276.png)

##### 随机梯度下降（SGD）：

原因：

   样本中的信息是冗余的
 足够的样本意味着我们可以承受噪音更新
 永无止境的流意味着我们不应该等待所有的数据
 跟踪非平稳数据意味着目标正在变化

![image-20211206141627863](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211206141627863.png)

随机梯度下降优势：

-  梯度易于计算（瞬时）
-  不太容易出现局部极小值
-  要使用的内存很小
-  迅速找到合理的解决方案
-  可用于更复杂的模型

缺点：

-  随机梯度的方差很大
-  迭代算法是不稳定的
-  难以达到高精度

##### 批量随机梯度下降

![image-20211206144749175](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211206144749175.png)

#### 多分类

三个常见的分类问题

-  二元分类
-  多类分类
-  多标签分类

三种常见分类：

-  转换为二进制分类

-  从二元分类扩展
-  层次分类

这些策略将多类分类问题简化为多个二元分类问题。

-  One-vs.-rest  


-  One-vs.-one 


-  Decision Directed Acyclic Graph, DDAG


One VS rest方法：
	1.方法
      训练：
      对于每个类：
      训练一个二分类器，将该内容作为正分类，其余的变成负分类


2. 预测：

  对于每个二分类器：
      产生一个真值信心得分（confidence score）
       预测最高得分下的那个label

3. 优缺点

​        优点：
​            训练K个二进制分类器以解决K路多类问题。
​         缺点：
  二元分类的分布是不平衡的。
  （负片集比正片集大得多。）
  置信值的范围可能因以下因素而异：
  二进制分类器。

One VS one 方法：
1. 方法
训练：
对于每个类的pair：
训练一个二分类器来去分辨两个类

2. 预测：
    对于每个二分类器：
    比较两个目录内容并且进行投票:

  ![image-20211206151905370](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211206151905370.png)

Predict the label with the maximum number of votes wins.预测这个计算的内容
yˆ=Max(A,B,C,D)

3. 优缺点：
(1)	优点：
The distributions of the binary classifications are balanced.
二元分类器的分离是平衡的
(2)	缺点：
①	Train K(K − 1)/2 binary classifiers for a K-way multi-class
problem, which has high computed complexity.
对K路分类问题，训练 K(K − 1)/2二分类器，这个能有高计算复杂度
②	Suffer from ambiguities when receive the same number of
votes.
当得到相同的投票的时候，需要忍受模棱两可的选择

DDAG：
1. 比起来一对一方法，此方法使用了一个根基础的二分有向无环图，这个有着内节点和叶子节点：
2. 预测：
    从根节点开始
    在到达叶子节点之前：
    评估二元决策函数
    根据输出值向左或者向右移动

比起来One VS one 方法：
每个二元分类器的相关性减少了预测的难度和代价

K-近邻
 计算距离
计算测试对象与模型中每个对象之间的距离
训练集
 找到邻居
获取K-最近的训练对象作为邻居
 对标签投票
根据最频繁的邻居类别对测试对象进行分类

优势与劣势
 优势
 该方法是一种非参数分类算法
 该算法可以自然地处理二进制和多类分类
 缺点
 计算和内存要求很高
 在对象之间找到良好的表示和距离度量是非常重要的
坚固的

层次分类：

label tree

训练：
 之前，叶节点只包含一个类
 每个父节点被划分为多个集群，一个用于
每个子节点
 在每个节点上，训练一个简单的分类器进行判别
在不同的子类集群之间

预测：
 从根节点开始
 移动到与标签关联的叶节点
优势与劣势
 有利条件
 树方法降低了预测的成本
 缺点
 依赖于良好的聚类方法。

# 四、逻辑回归

#### 逻辑回归

sigmod函数：
$$
g(z)=\frac{1}{1+ e^{-z}}
$$
g(z)在0、1之间，单调增 g(-z)=1-g(z)

**逻辑回归的损失函数不适宜用最小二乘法**

##### ==**以下的逻辑回归损失函数推导基于标签y属于{-1，1}**==

新损失函数：
$$
损失函数：\\
L(w)=\frac 1 n \sum_{i=1}^n log(1+e^{-y_iw^Tx_i})
$$
![image-20211210112324775](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210112324775.png)

利用最大似然估计：

![image-20211210110307367](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210110307367.png)

![image-20211210110826087](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210110826087.png)

与支持向量机类似，采用正则化避免过拟合：
$$
J(w)=L(w)+\frac{\lambda}{2} \|w\|^2\\
\lambda 是正则化参数
$$
||w'||方叫做L2正则化

为什么正则化？

- 防止模型简单
- 防止过拟合

与SVM联系：

- 都是监督学习
- 二进制分类

![image-20211210111855958](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210111855958.png)

##### **如果标签y属于{0，1}**

![image-20211210113501154](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210113501154.png)

同样采用最大似然估计：

![image-20211210113916738](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210113916738.png)

同上，加上$\frac{\lambda}{2}||w||^2$ 避免正则化

​      ![image-20211210115819159](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210115819159.png)

![image-20211210115848501](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210115848501.png)

#### Softmax Regression

##### 多分类问题

$$
y_i\in\{0.1.\cdots ,k-1\}\\
令第j个概率为o_j\\
则\sum_jo_j=1
$$

![image-20211210163855127](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210163855127.png)

w是权重
$$
softmax函数（归一化函数）：\frac{e^{z_j}}{\sum_{l=0}^{K-1}e^{z_l}}
$$
![image-20211210170459366](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210170459366.png)

同上，加上$\frac{\lambda}{2}||w||^2$ 避免正则化

梯度下降：

![image-20211210172233799](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210172233799.png)

多分类问题 当K=2时，等价于逻辑回归

证明：

![image-20211210172656077](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210172656077.png)

![image-20211210172709111](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210172709111.png)

![image-20211210172731507](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210172731507.png)

#### Large-Margin Softmax Loss（这以后不考）

![image-20211210174206953](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210174206953.png)

![image-20211210174229384](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210174229384.png)

![image-20211210174256879](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210174256879.png)

![image-20211210174320193](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210174320193.png)

![image-20211210174334695](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210174334695.png)

**Large-Margin Softmax Loss比Softmax Loss约束更强**

![image-20211210174721544](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210174721544.png)

![image-20211210174740812](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210174740812.png)

##### Angular Softmax Loss

![image-20211210175633811](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210175633811.png)

![image-20211210175647914](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210175647914.png)

![image-20211210175742162](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211210175742162.png)

# 五、欠拟合过拟合交叉验证

训练集：拟合模型的数据

测试集：用于评估模型的数据

大约七三分

分成三份 可以再分一个验证集

**为什么我们需要验证集？**
 *商业原因*
 需要选择最好的型号
 所选型号的测量精度/功率
 更好地测量建模项目的ROI
 *统计原因*
 建模技术本质上是为了
尽量减少“损失”或“偏差”
 在某种程度上，模型将始终与“噪波”相匹配
“信号”
 如果你只是在一个给定的数据集上拟合了一组模型
选择“最好”的一个，它可能会过于“乐观”

正常拟合：模型拟合可以捕捉数据未来趋势

欠拟合：模型无法捕获数据的基本趋势

过拟合：该模型过于复杂，无法捕捉真实趋势，该模型试图拟合数据的噪声或异常值

判断？
 欠拟合：如果训练集的误差较大，而泛化误差较大
	 需要增加容量（模型的复杂性）
 过拟合：如果训练集的误差相对较小，而泛化误差较大
	 需要降低容量（模型的复杂性）
	 或者增加训练集

![image-20211211163753571](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211211163753571.png)

复杂模型
 过于复杂可能会降低模型的精度
未来数据（称为偏差-方差权衡）
 低偏差
 模型与训练数据吻合良好
 高方差
 模型更可能做出错误的预测

##### K-折验证

如果我们想减少数据的可变性
我们可以使用多轮交叉验证
不同分区
然后，对所有回合的结果进行平均
给定一个数据𝑆 从人群中抽样𝐷

![image-20211211165144515](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211211165144515.png)

![image-20211211165158517](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211211165158517.png)

# 六、集成方法

集成学习：将几个基本模型结合起来，生成一个更好的预测模型

主要：Bagging，Boosting

**Bagging**

 通过bootstrap采样获得T个采样集
 分别通过样本集对T-base学习者进行训练
 分类：
得票最多的类成为最终类
 对于回归：
最终输出是每个基础学习者的平均输出

**随机森林：**

随机森林是bagging的一个扩展，使用决策树作为基础学习器

从p个特征中随机选择m个，以获得最佳分割特征

对比：

套袋与随机林的比较
 随机林的训练效率优于bagging
 Bagging使用具有明确结构的决策树
 随机森林使用具有随机结构的决策树

**决策树：**

在以下情况下，属性是好的：
 对于一个值，所有实例都是正值
 对于其他值，所有实例都是负值
在以下情况下，属性为差：
 它没有规定歧视
 属性对决策不重要
 每个值的正实例数和负实例数相同



**熵值**
$$
Entropy(D)=-p_+log_2p_+-p_-log_2p_-
$$
**信息增益（gain）**

![image-20211211171851282](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211211171851282.png)

选择信息增益大的作为重要指标

#### Adaboost

刚开始给相同的权重1/n

分类正确给低权重 错误给高权重

更新权重
$$
\alpha_t=\frac 1 2ln\frac{1-e_t}{e_t}
$$
更新权重：
$$
w_{t+1}(i)=\frac{w_t(i)}{z_t}e^{-\alpha _ty_ih_t(x_i)}=2\sqrt{e_t(1-e_t)}
$$
![s](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211211172828944.png)

![image-20211211172851246](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211211172851246.png)

为什么adaboost？

- ​	只需要一个简单的分类器作为基础学习者
- ​    可以实现类似于强大分类器的预测 

- ​    可以与任何学习算法相结合
- ​    只需要很少的参数调整
- ​    扩展到二进制分类以外的问题

# 七、聚类

监督学习：对于样本x，有固定的标签值，比如：线性回归、逻辑回归、贝叶斯、svm等

无监督学习：没有目标值。目标：理解数据、总结数据、确定概念。

![image-20211212110444350](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212110444350.png)

样本间的相似性：![image-20211212110715575](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212110715575.png)

##### K-means聚类(前面分配$O(nKm)$ 后面质心更新$O(nm)$)

![image-20211212111609174](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212111609174.png)

![image-20211212111808289](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212111808289.png)

​     

![image-20211212115231319](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212115231319.png)

简便易行的方法

参数的有效数量为𝑚 × 𝐾, 𝑚 是样本尺寸，𝐾 是群集的数量

如果线性决策边界失败，则不可用

对于k的选择：

较小：可以提供更好的解释

较大：如果将聚类用于特征提取，则此选项非常有用

没有原则性的方法可以做到这一点。一种启发式方法是根据K绘制损失图，并在图中寻找“膝盖”

##### HAC层次凝聚类算法（高维度效果不好，但更灵活）（O（n^2 m)（不考）

![image-20211212120415672](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212120415672.png)

问：

哪个距离会倾向于将大型集群与每个集群合并
另外
答：最小，因为较大的簇更可能有一对
接近的样本数量
 哪个距离会产生“连锁效应”并领先
长串簇？
A:min，因为只有一个距离必须很小
 哪种距离更倾向于紧凑型集群？
答：最大，因为所有的距离都必须很小才能合并

合并方式：min、max、centroid、average

平均距离（average）和质心（centroid）距离提供了折衷方案
在最小值和最大值之间
 非参数。可以得到任意的簇形状
 在具有不相关的高维空间中可能过度适合

# 八、降维的主成分分析（PCA）

原因：

- 数据可能包含非常相似甚至相同的列
- 一些列可能有噪音
- 为了数据可视化
- 维度降低
- 分布式表示

方差为：
$$
x^TSx\\
S为协方差
$$
证明：

![image-20211215110559488](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211215110559488.png)

![image-20211212163944213](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212163944213.png)

![image-20211212164413158](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212164413158.png)

![image-20211212164500150](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212164500150.png)

# 九、推荐系统（不考）

**推荐系统将统计和知识发现技术应用于产品推荐问题。**
**推荐系统的优点：**
 提高转化率：
帮助客户找到他们想要购买的东西。
 交叉销售：
建议额外的产品。
 提高客户忠诚度：
创建增值关系

**CF算法的类型**（协同过滤）
 基于内存的CF：利用整个用户项数据库
生成预测。
 基于用户的CF：找到相似的用户来预测评分。
 基于项目的CF：使用类似项目预测评分。
 基于模型的CF：根据评级数据（矩阵）构建模型
因子分解等），并使用此模型预测缺失评级。

##### 基于模型的CF：

模型是从底层数据而不是启发式学习的
用户评级（或购买）模型：
 矩阵分解
 聚类（分类）
 关联规则
 基于深度学习的方法
 其他型号
矩阵分解是应用最广泛的算法。

# 十、图像处理

数码相机用一个传感器阵列保存影像

阵列中的每个单元都是光敏二极管，可以将光子转换为电子

两种常见类型：电荷耦合器件（CCD）和CMOS

每个传感器单元记录以小范围方向进入的光量

光栅图像（像素矩阵）：

主要因素
•照明强度和方向
•表面几何
•表面材料
•附近表面
•摄像机增益/曝光

color space：

- RGB
- HSV(色调（H），饱和度（S），明度（V）)
- YCbCr
- CIE L*a*b

什么是图像处理？

- 提高图像质量
-  改善人类判读的图像信息
-  处理有助于最大限度地提高清晰度、清晰度和细节
- 感兴趣的特征对信息提取的影响

**图像去噪**
释义
 图像去噪是指在图像中降低噪声的过程
数字图像称为图像去噪
去噪方法
 中值滤波
 高斯滤波
 均值滤波
 维纳滤波
 傅里叶滤波

**图像增强**
释义
 增强图像中的有用信息
 这可能是一个扭曲的过程
 对于给定的图像，它可以改善图像的视觉效果
应用
图像增强方法
 基于空域的算法
 点算法
 邻域增强算法
 基于频域的算法

**图像去模糊**（deblur）
 造成图像模糊的原因有很多，包括光学因素，
大气因素、人为因素、技术因素等。
 为了达到更好的处理效果，不同的颜色会造成模糊
原因往往需要不同的处理方法
 从技术角度来看，模糊图像处理方法是可行的
主要分为三类：
•图像增强
•图像恢复
•超分辨率重建

**图像修补（inpaint）**
释义
 图像复原是指重建图像的过程
以及视频中丢失或损坏的部分
 图像恢复，也称为图像插值或视频
插值
 使用复杂的算法替换丢失的、损坏的
图像数据，主要用来替换一些小区域和瑕疵

**纹理合成（texture synthesis）**
释义
 为了解决接缝问题，提出了纹理合成方法
纹理贴图中的锯齿和其他问题
 目前，纹理合成方法可分为两种
类别：
•一个是过程纹理合成（PTS）
-它通过模拟曲面直接在曲面上生成纹理
物理生成过程
•另一种是基于样本的纹理合成（SBTS）
-给定一个小纹理，生成一个大的相似纹理

#### 卷积（convolution）

卷积mask
 mask是一种矩阵，通常大小为1x1、3x3、5x5或7x7（奇数
数字）。
 卷积步骤：
•水平和垂直翻转mask（对角线翻转）
•将mask滑到图像上（中心对齐）
•将相应的元素相乘并相加
•重复此操作，直到计算出所有图像值

![image-20211212173056993](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211212173056993.png)

box filter作用
•将每个像素替换为其邻域的平均值
•达到平滑效果（去除尖锐特征

# 十一、神经网络和深度学习

#### 神经网络

![image-20211213110211723](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213110211723.png)

![image-20211213110425899](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213110425899.png)

输入值进入神经元会权重+偏置，然后再通过激活函数激活（sigmoid）

![image-20211213110542369](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213110542369.png)

浅层网络需要比深层网络多得指数级的神经元才能达到函数逼近的相同精度

#### **正向传播**

![image-20211213111245221](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213111245221.png)

#### 反向传播

![image-20211213112412439](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213112412439.png)

![image-20211213112429114](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213112429114.png)

![image-20211213112717609](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213112717609.png)

反向传播将误差/损失从最后一层传播到第一层，以计算每一层的梯度

### 卷积神经网络CNN

内核/过滤器是一个称为权重的值矩阵（同卷积矩阵的filter）

卷积过程同之前部分

![image-20211213114402778](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213114402778.png)

输入层-卷积层-输出层

批量规范化：

![image-20211213160810843](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213160810843.png)

![image-20211213160837277](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213160837277.png)

![image-20211213160916024](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213160916024.png)

激励函数：

![image-20211213160949926](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213160949926.png)

池化层：

![image-20211213161020515](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213161020515.png)

![image-20211213161410870](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213161410870.png)

深度学习应用

- 图像分类；目标检测；图像分割
  行人检测；车道检测；交通标志识别

classical cnns

- LeNet  手写数字识别
- AlexNet  使用RELU 在imagenet训练
- ResNet

# 十二、序列化建模

序列建模是对任何类型的序列数据（如音频、文本等）进行建模、解释、预测或生成的任务。

应用

- 语音识别
- 机器翻译
- 视频字幕
- 打字自动填补
- 关键字识别

RNN

**为了处理序列输入（前后输入有关联）**

RNN可以被认为是同一网络的多个副本，每个副本向后续网络传递一条消息。

![image-20211213162504393](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213162504393.png)

![image-20211213162626287](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213162626287.png)

![image-20211213162811324](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213162811324.png)

​	LSTM

处理前后距离较远的关联

![image-20211213163432163](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213163432163.png)

![image-20211213163607624](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213163607624.png)

 By introducing gates and cell state 𝐶𝑡 , LSTMs are able to keep previous  information for long time and deal with long-term dependencies.

![image-20211213163654050](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213163654050.png)

![image-20211213163716467](C:\Users\86130\AppData\Roaming\Typora\typora-user-images\image-20211213163716467.png)